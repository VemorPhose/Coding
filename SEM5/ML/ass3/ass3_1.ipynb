{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298180c3",
   "metadata": {},
   "source": [
    "### 3.1. Gradient Descent on Salary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bf571",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "76368eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38a6c2",
   "metadata": {},
   "source": [
    "### 2. Get and separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5358fe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1  1.3  1.5  2.   2.2  2.9  3.   3.2  3.2  3.7  3.9  4.   4.   4.1\n",
      "  4.5  4.9  5.1  5.3  5.9  6.   6.8  7.1  7.9  8.2  8.7  9.   9.5  9.6\n",
      " 10.3 10.5]\n",
      "[ 39343.  46205.  37731.  43525.  39891.  56642.  60150.  54445.  64445.\n",
      "  57189.  63218.  55794.  56957.  57081.  61111.  67938.  66029.  83088.\n",
      "  81363.  93940.  91738.  98273. 101302. 113812. 109431. 105582. 116969.\n",
      " 112635. 122391. 121872.]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('salary_data.csv', delimiter=',', usecols=(0, 1), skiprows=1)\n",
    "X = data[:, 0]\n",
    "y = data[:, 1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c49849",
   "metadata": {},
   "source": [
    "### 3. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0031d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3, Test size: 27\n",
      "Train size: 6, Test size: 24\n",
      "Train size: 9, Test size: 21\n",
      "Train size: 12, Test size: 18\n",
      "Train size: 15, Test size: 15\n",
      "Train size: 18, Test size: 12\n",
      "Train size: 21, Test size: 9\n",
      "Train size: 24, Test size: 6\n",
      "Train size: 27, Test size: 3\n"
     ]
    }
   ],
   "source": [
    "splits = []\n",
    "for i in range(9):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-0.1*(i+1), random_state=42)\n",
    "    print(f'Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}')\n",
    "    splits.append((X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8da96",
   "metadata": {},
   "source": [
    "### 4. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "20074a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b): \n",
    "    m = x.shape[0] \n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(w, x[i]) + b\n",
    "        total_cost += (f_wb - y[i]) ** 2\n",
    "    total_cost /= 2 * m\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02c23bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(w, x[i]) + b\n",
    "        dj_dw += (f_wb - y[i]) * x[i]\n",
    "        dj_db += f_wb - y[i]\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1d189a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, verbose=True): \n",
    "    m = len(x)\n",
    "    \n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b )  \n",
    "\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(x, y, w, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        if verbose and i % math.ceil(num_iters/100) == 0:\n",
    "            w_history.append(w)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "        # early stopping using delta cost\n",
    "        if i > 0 and abs(J_history[-1] - J_history[-2]) < 1e-3:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    return w, b, J_history, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc8f93",
   "metadata": {},
   "source": [
    "### 5. Test gradient descent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a0b6bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = splits[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f160bedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "Cost at initial w: 2863557882.699\n"
     ]
    }
   ],
   "source": [
    "# Compute cost with some initial values for paramaters w, b\n",
    "initial_w = 2\n",
    "initial_b = 1\n",
    "\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6fcbad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at initial w, b (zeros): -413134.0809523809 -71057.33333333333\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient with w initialized to zeroes\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a8f63eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 2691501095.52   \n",
      "Iteration 1000: Cost 57364145.40   \n",
      "Iteration 2000: Cost 41207544.10   \n",
      "Iteration 3000: Cost 31135261.76   \n",
      "Iteration 4000: Cost 24856040.69   \n",
      "Iteration 5000: Cost 20941474.37   \n",
      "Iteration 6000: Cost 18501071.43   \n",
      "Iteration 7000: Cost 16979685.41   \n",
      "Iteration 8000: Cost 16031229.16   \n",
      "Iteration 9000: Cost 15439946.44   \n",
      "Iteration 10000: Cost 15071331.39   \n",
      "Iteration 11000: Cost 14841530.90   \n",
      "Iteration 12000: Cost 14698269.61   \n",
      "Iteration 13000: Cost 14608958.25   \n",
      "Iteration 14000: Cost 14553280.13   \n",
      "Iteration 15000: Cost 14518569.50   \n",
      "Iteration 16000: Cost 14496930.34   \n",
      "Iteration 17000: Cost 14483440.14   \n",
      "Iteration 18000: Cost 14475030.14   \n",
      "Iteration 19000: Cost 14469787.21   \n",
      "Iteration 20000: Cost 14466518.69   \n",
      "Iteration 21000: Cost 14464481.03   \n",
      "Iteration 22000: Cost 14463210.73   \n",
      "Iteration 23000: Cost 14462418.80   \n",
      "Iteration 24000: Cost 14461925.10   \n",
      "Iteration 25000: Cost 14461617.32   \n",
      "Iteration 26000: Cost 14461425.45   \n",
      "Iteration 27000: Cost 14461305.83   \n",
      "Iteration 28000: Cost 14461231.26   \n",
      "Iteration 29000: Cost 14461184.77   \n",
      "Iteration 30000: Cost 14461155.78   \n",
      "Iteration 31000: Cost 14461137.72   \n",
      "Iteration 32000: Cost 14461126.45   \n",
      "Iteration 33000: Cost 14461119.43   \n",
      "Iteration 34000: Cost 14461115.05   \n",
      "Iteration 35000: Cost 14461112.32   \n",
      "Iteration 36000: Cost 14461110.62   \n",
      "Early stopping at iteration 36606\n",
      "(w,b) found by gradient descent: (9339.7422,25914.2585)\n"
     ]
    }
   ],
   "source": [
    "# test run\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "iterations = 100000\n",
    "tmp_alpha = 0.001\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(X_train, y_train, w_init, b_init, compute_cost, compute_gradient, tmp_alpha, iterations)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b946870",
   "metadata": {},
   "source": [
    "### 6. Find and Save Parameters for all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6d8bae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(w,b) found by gradient descent for pct = 0.10: (13184.9772,11788.9591)\n",
      "(w,b) found by gradient descent for pct = 0.20: (12275.8822,13995.6238)\n",
      "(w,b) found by gradient descent for pct = 0.30: (10098.0898,22077.0060)\n",
      "(w,b) found by gradient descent for pct = 0.40: (9985.3764,22434.8584)\n",
      "(w,b) found by gradient descent for pct = 0.50: (9616.1976,25191.4029)\n",
      "(w,b) found by gradient descent for pct = 0.60: (9736.4901,23840.7755)\n",
      "(w,b) found by gradient descent for pct = 0.70: (9693.9244,23672.8649)\n",
      "(w,b) found by gradient descent for pct = 0.80: (9769.3088,22991.8389)\n",
      "(w,b) found by gradient descent for pct = 0.90: (9767.5589,23515.1411)\n"
     ]
    }
   ],
   "source": [
    "parameters = []\n",
    "\n",
    "for split in splits:\n",
    "    X_train, X_test, y_train, y_test = split\n",
    "    pct = X_train.shape[0] / (X_train.shape[0] + X_test.shape[0])\n",
    "    w_init = 0\n",
    "    b_init = 0\n",
    "    iterations = 10000\n",
    "    tmp_alpha = 0.001\n",
    "    # run gradient descent\n",
    "    w_final, b_final, J_hist, p_hist = gradient_descent(X_train, y_train, w_init, b_init, compute_cost, compute_gradient, tmp_alpha, iterations, verbose=False)\n",
    "    print(f\"(w,b) found by gradient descent for pct = {pct:.2f}: ({w_final:8.4f},{b_final:8.4f})\")\n",
    "    parameters.append((w_final, b_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd0294",
   "metadata": {},
   "source": [
    "### 7. Evaluation Functions and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "17a19805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w0, w1, x):\n",
    "    return w0 + w1 * x\n",
    "\n",
    "def rss(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = rss(y_true, y_pred)\n",
    "    ss_tot = np.sum((y_true - y_true.mean())**2)\n",
    "    return 1 - ss_res/ss_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab86a98",
   "metadata": {},
   "source": [
    "### 8. Plots and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "02150373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "param_rows = []\n",
    "metric_rows = []\n",
    "pred_rows = []\n",
    "line_colors = plt.cm.viridis(np.linspace(0, 1, 9))\n",
    "feature_col = 'YearsExperience'\n",
    "target_col = 'Salary'\n",
    "PLOT_DIR = 'ass3_1/plots'\n",
    "\n",
    "# For overlay plot of all hypotheses\n",
    "fig_all, ax_all = plt.subplots(figsize=(7,5))\n",
    "ax_all.set_title('Regression Lines Across Training Splits')\n",
    "ax_all.set_xlabel(feature_col)\n",
    "ax_all.set_ylabel(target_col)\n",
    "ax_all.scatter(X, y, s=25, c='lightgray', label='All data')\n",
    "\n",
    "for theta, split in zip(parameters, splits):\n",
    "    w1, w0 = theta\n",
    "    X_train, X_test, y_train, y_test = split\n",
    "    y_train_pred = predict(w0, w1, X_train)\n",
    "    y_test_pred = predict(w0, w1, X_test)\n",
    "\n",
    "    train_rss = rss(y_train, y_train_pred)\n",
    "    test_rss = rss(y_test, y_test_pred)\n",
    "    train_rss_mean = train_rss / len(y_train)\n",
    "    test_rss_mean = test_rss / len(y_test)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    pct = int(len(X_train) / (len(X_train) + len(X_test)) * 100)\n",
    "\n",
    "    param_rows.append({\n",
    "        'train_pct': pct,\n",
    "        'w0': w0,\n",
    "        'w1': w1,\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test)\n",
    "    })\n",
    "\n",
    "    metric_rows.append({\n",
    "        'train_pct': pct,\n",
    "        'train_rss_mean': train_rss_mean,\n",
    "        'test_rss_mean': test_rss_mean,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2\n",
    "    })\n",
    "\n",
    "    for xv, yv, yhat in zip(X_train, y_train, y_train_pred):\n",
    "        pred_rows.append({\n",
    "            'train_pct': pct,\n",
    "            'set': 'train',\n",
    "            'x': xv,\n",
    "            'y': yv,\n",
    "            'y_hat': yhat\n",
    "        })\n",
    "\n",
    "    for xv, yv, yhat in zip(X_test, y_test, y_test_pred):\n",
    "        pred_rows.append({\n",
    "            'train_pct': pct,\n",
    "            'set': 'test',\n",
    "            'x': xv,\n",
    "            'y': yv,\n",
    "            'y_hat': yhat\n",
    "        })\n",
    "\n",
    "    # Individual plot for this split\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.scatter(X_train, y_train, c='blue', alpha=0.7, label='Train')\n",
    "    ax.scatter(X_test, y_test, c='orange', alpha=0.7, label='Test')\n",
    "    # line over full feature range\n",
    "    x_line = np.linspace(X.min(), X.max(), 100)\n",
    "    y_line = predict(w0, w1, x_line)\n",
    "    ax.plot(x_line, y_line, color='red', label=f'Hypothesis (w0={w0:.2f}, w1={w1:.2f})')\n",
    "    ax.set_title(f'Train % = {pct}')\n",
    "    ax.set_xlabel(feature_col)\n",
    "    ax.set_ylabel(target_col)\n",
    "    ax.legend()\n",
    "    fname = os.path.join(PLOT_DIR, f'hypothesis_{pct}pct.png')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fname, dpi=120)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Add line to overlay plot\n",
    "    ax_all.plot(x_line, y_line, color=line_colors[i], label=f'{pct}% (w1={w1:.2f})')\n",
    "\n",
    "# Finish overlay plot\n",
    "ax_all.legend(fontsize='small', ncol=2)\n",
    "fig_all.tight_layout()\n",
    "fig_all.savefig(os.path.join(PLOT_DIR, 'all_hypotheses.png'), dpi=130)\n",
    "plt.close(fig_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3120cf30",
   "metadata": {},
   "source": [
    "### 9. Saving CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26d1a7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ass3_1/parameters.csv, ass3_1/metrics.csv, ass3_1/results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "SAVE_DIR = 'ass3_1'\n",
    "\n",
    "params_df = pd.DataFrame(param_rows)\n",
    "metrics_df = pd.DataFrame(metric_rows)\n",
    "predictions_df = pd.DataFrame(pred_rows)\n",
    "\n",
    "params_csv = f'{SAVE_DIR}/parameters.csv'\n",
    "metrics_csv = f'{SAVE_DIR}/metrics.csv'\n",
    "predictions_csv = f'{SAVE_DIR}/results.csv'\n",
    "\n",
    "params_df.to_csv(params_csv, index=False)\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "predictions_df.to_csv(predictions_csv, index=False)\n",
    "\n",
    "print(f'Saved {params_csv}, {metrics_csv}, {predictions_csv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b89136",
   "metadata": {},
   "source": [
    "### 10. Training pct vs RSS and R2 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7b283630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved performance plots.\n"
     ]
    }
   ],
   "source": [
    "fig_rss, ax_rss = plt.subplots(figsize=(6,4))\n",
    "ax_rss.plot(metrics_df['train_pct'], metrics_df['train_rss_mean'], marker='o', label='Train Mean RSS')\n",
    "ax_rss.plot(metrics_df['train_pct'], metrics_df['test_rss_mean'], marker='s', label='Test Mean RSS')\n",
    "ax_rss.set_xlabel('Training %')\n",
    "ax_rss.set_ylabel('Mean RSS')\n",
    "ax_rss.set_title('Training % vs Mean RSS')\n",
    "ax_rss.legend()\n",
    "fig_rss.tight_layout()\n",
    "fig_rss.savefig(os.path.join(PLOT_DIR, 'training_pct_vs_mean_rss.png'), dpi=130)\n",
    "plt.close(fig_rss)\n",
    "\n",
    "fig_r2, ax_r2 = plt.subplots(figsize=(6,4))\n",
    "ax_r2.plot(metrics_df['train_pct'], metrics_df['train_r2'], marker='o', label='Train R2')\n",
    "ax_r2.plot(metrics_df['train_pct'], metrics_df['test_r2'], marker='s', label='Test R2')\n",
    "ax_r2.set_xlabel('Training %')\n",
    "ax_r2.set_ylabel('R^2')\n",
    "ax_r2.set_title('Training % vs R^2')\n",
    "ax_r2.legend()\n",
    "fig_r2.tight_layout()\n",
    "fig_r2.savefig(os.path.join(PLOT_DIR, 'training_pct_vs_r2.png'), dpi=130)\n",
    "plt.close(fig_r2)\n",
    "\n",
    "print('Saved performance plots.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
