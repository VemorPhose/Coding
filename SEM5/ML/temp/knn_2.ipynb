{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "### 1.a. Read the dataset\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(a): READING THE IRIS DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(f\"\\nSpecies distribution:\")\n",
    "print(df['Species'].value_counts())\n",
    "print()\n",
    "\n",
    "### 1.b. Normalize the features\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(b): FEATURE NORMALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract features and target\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
    "y = df['Species'].values\n",
    "\n",
    "print(f\"\\nOriginal feature ranges:\")\n",
    "for i, col in enumerate(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']):\n",
    "    print(f\"  {col}: [{X[:, i].min():.2f}, {X[:, i].max():.2f}]\")\n",
    "\n",
    "# Min-Max Normalization\n",
    "X_normalized = np.zeros_like(X)\n",
    "for i in range(X.shape[1]):\n",
    "    min_val = X[:, i].min()\n",
    "    max_val = X[:, i].max()\n",
    "    X_normalized[:, i] = (X[:, i] - min_val) / (max_val - min_val)\n",
    "\n",
    "print(f\"\\nNormalized feature ranges:\")\n",
    "for i, col in enumerate(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']):\n",
    "    print(f\"  {col}: [{X_normalized[:, i].min():.2f}, {X_normalized[:, i].max():.2f}]\")\n",
    "print()\n",
    "\n",
    "### 1.c. Split the dataset (80:20)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(c): TRAIN-TEST SPLIT (80:20)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Manual train-test split\n",
    "n_samples = len(X_normalized)\n",
    "n_train = int(0.8 * n_samples)\n",
    "\n",
    "# Shuffle indices\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "test_indices = indices[n_train:]\n",
    "\n",
    "X_train = X_normalized[train_indices]\n",
    "X_test = X_normalized[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(f\"\\nTotal samples: {n_samples}\")\n",
    "print(f\"Training samples: {len(X_train)} ({len(X_train)/n_samples*100:.1f}%)\")\n",
    "print(f\"Testing samples: {len(X_test)} ({len(X_test)/n_samples*100:.1f}%)\")\n",
    "print(f\"\\nTraining set species distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for species, count in zip(unique, counts):\n",
    "    print(f\"  {species}: {count}\")\n",
    "print(f\"\\nTest set species distribution:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for species, count in zip(unique, counts):\n",
    "    print(f\"  {species}: {count}\")\n",
    "print()\n",
    "\n",
    "### 1.d. Implement K-NN Classification\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(d): K-NN CLASSIFICATION IMPLEMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test, k):\n",
    "    \"\"\"\n",
    "    K-NN classification without weights.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_test: Test features\n",
    "        k: Number of neighbors\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Predicted labels for test set\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for test_point in X_test:\n",
    "        # Calculate distances to all training points\n",
    "        distances = []\n",
    "        for i, train_point in enumerate(X_train):\n",
    "            dist = euclidean_distance(test_point, train_point)\n",
    "            distances.append((dist, y_train[i]))\n",
    "        \n",
    "        # Sort by distance and get k nearest neighbors\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        k_nearest = distances[:k]\n",
    "        \n",
    "        # Get labels of k nearest neighbors\n",
    "        k_nearest_labels = [label for _, label in k_nearest]\n",
    "        \n",
    "        # Majority voting\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        predictions.append(most_common[0][0])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Select 5 random K values where K <= sqrt(n)\n",
    "n = len(X_train)\n",
    "max_k = int(np.sqrt(n))\n",
    "print(f\"\\nNumber of training samples (n): {n}\")\n",
    "print(f\"Maximum K value (√n): {max_k}\")\n",
    "\n",
    "# Generate 5 random K values\n",
    "k_values = sorted(np.random.choice(range(1, max_k + 1), size=5, replace=False))\n",
    "print(f\"Selected K values: {k_values}\")\n",
    "print()\n",
    "\n",
    "### 1.e. Compute Metrics and Visualize Results\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(e): STANDARD K-NN PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and accuracy for multi-class classification.\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_true)\n",
    "    \n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "    \n",
    "    for cls in classes:\n",
    "        # True Positives: correctly predicted as cls\n",
    "        tp = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        \n",
    "        # False Positives: incorrectly predicted as cls\n",
    "        fp = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        \n",
    "        # False Negatives: incorrectly predicted as not cls\n",
    "        fn = np.sum((y_pred != cls) & (y_true == cls))\n",
    "        \n",
    "        # Precision and Recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        precision_dict[cls] = precision\n",
    "        recall_dict[cls] = recall\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    \n",
    "    return precision_dict, recall_dict, accuracy\n",
    "\n",
    "# Store results for standard K-NN\n",
    "standard_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nEvaluating K={k}...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = knn_predict(X_train, y_train, X_test, k)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision_dict, recall_dict, accuracy = compute_metrics(y_test, y_pred)\n",
    "    \n",
    "    standard_results[k] = {\n",
    "        'predictions': y_pred,\n",
    "        'precision': precision_dict,\n",
    "        'recall': recall_dict,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision per class: {precision_dict}\")\n",
    "    print(f\"  Recall per class: {recall_dict}\")\n",
    "\n",
    "# Create visualization\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# 1. Accuracy vs K\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "accuracies = [standard_results[k]['accuracy'] for k in k_values]\n",
    "plt.plot(k_values, accuracies, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Standard K-NN: Accuracy vs K')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# 2. Precision per class\n",
    "plt.subplot(2, 3, 2)\n",
    "species = list(standard_results[k_values[0]]['precision'].keys())\n",
    "for cls in species:\n",
    "    precisions = [standard_results[k]['precision'][cls] for k in k_values]\n",
    "    plt.plot(k_values, precisions, 'o-', label=cls, linewidth=2, markersize=8)\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Standard K-NN: Precision per Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# 3. Recall per class\n",
    "plt.subplot(2, 3, 3)\n",
    "for cls in species:\n",
    "    recalls = [standard_results[k]['recall'][cls] for k in k_values]\n",
    "    plt.plot(k_values, recalls, 'o-', label=cls, linewidth=2, markersize=8)\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Standard K-NN: Recall per Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# 4. Average Precision and Recall\n",
    "plt.subplot(2, 3, 4)\n",
    "avg_precisions = [np.mean(list(standard_results[k]['precision'].values())) for k in k_values]\n",
    "avg_recalls = [np.mean(list(standard_results[k]['recall'].values())) for k in k_values]\n",
    "plt.plot(k_values, avg_precisions, 'o-', label='Avg Precision', linewidth=2, markersize=8)\n",
    "plt.plot(k_values, avg_recalls, 's-', label='Avg Recall', linewidth=2, markersize=8)\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Standard K-NN: Average Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# 5. Confusion Matrix for best K\n",
    "best_k_standard = max(k_values, key=lambda k: standard_results[k]['accuracy'])\n",
    "plt.subplot(2, 3, 5)\n",
    "y_pred_best = standard_results[best_k_standard]['predictions']\n",
    "conf_matrix = np.zeros((len(species), len(species)), dtype=int)\n",
    "for i, true_cls in enumerate(species):\n",
    "    for j, pred_cls in enumerate(species):\n",
    "        conf_matrix[i, j] = np.sum((y_test == true_cls) & (y_pred_best == pred_cls))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=species, yticklabels=species)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (K={best_k_standard})')\n",
    "\n",
    "# 6. Bar plot of metrics for best K\n",
    "plt.subplot(2, 3, 6)\n",
    "x = np.arange(len(species))\n",
    "width = 0.35\n",
    "precisions_best = [standard_results[best_k_standard]['precision'][cls] for cls in species]\n",
    "recalls_best = [standard_results[best_k_standard]['recall'][cls] for cls in species]\n",
    "plt.bar(x - width/2, precisions_best, width, label='Precision')\n",
    "plt.bar(x + width/2, recalls_best, width, label='Recall')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Score')\n",
    "plt.title(f'Metrics per Class (K={best_k_standard})')\n",
    "plt.xticks(x, species, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('1e_standard_knn_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest K value (standard K-NN): {best_k_standard}\")\n",
    "print(f\"Best accuracy: {standard_results[best_k_standard]['accuracy']:.4f}\")\n",
    "print()\n",
    "\n",
    "### 1.f. Distance-Weighted K-NN\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(f): DISTANCE-WEIGHTED K-NN IMPLEMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def weighted_knn_predict(X_train, y_train, X_test, k, weight_func='inverse'):\n",
    "    \"\"\"\n",
    "    Distance-weighted K-NN classification.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_test: Test features\n",
    "        k: Number of neighbors\n",
    "        weight_func: 'inverse' for 1/d or 'inverse_square' for 1/d²\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Predicted labels for test set\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for test_point in X_test:\n",
    "        # Calculate distances to all training points\n",
    "        distances = []\n",
    "        for i, train_point in enumerate(X_train):\n",
    "            dist = euclidean_distance(test_point, train_point)\n",
    "            distances.append((dist, y_train[i]))\n",
    "        \n",
    "        # Sort by distance and get k nearest neighbors\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        k_nearest = distances[:k]\n",
    "        \n",
    "        # Calculate weighted votes\n",
    "        class_weights = {}\n",
    "        for dist, label in k_nearest:\n",
    "            # Avoid division by zero\n",
    "            if dist == 0:\n",
    "                # If exact match, return that class immediately\n",
    "                predictions.append(label)\n",
    "                break\n",
    "            \n",
    "            if weight_func == 'inverse':\n",
    "                weight = 1.0 / dist\n",
    "            elif weight_func == 'inverse_square':\n",
    "                weight = 1.0 / (dist ** 2)\n",
    "            else:\n",
    "                weight = 1.0  # fallback\n",
    "            \n",
    "            if label in class_weights:\n",
    "                class_weights[label] += weight\n",
    "            else:\n",
    "                class_weights[label] = weight\n",
    "        else:\n",
    "            # Get class with maximum weight\n",
    "            predicted_class = max(class_weights.items(), key=lambda x: x[1])[0]\n",
    "            predictions.append(predicted_class)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "print(f\"\\nWeight functions:\")\n",
    "print(f\"  1. w = 1/d (inverse distance)\")\n",
    "print(f\"  2. w = 1/d² (inverse square distance)\")\n",
    "print(f\"\\nUsing same K values: {k_values}\")\n",
    "print()\n",
    "\n",
    "### 1.g. Weighted K-NN Performance Evaluation\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(g): WEIGHTED K-NN PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store results for weighted K-NN\n",
    "weighted_results = {\n",
    "    'inverse': {},\n",
    "    'inverse_square': {}\n",
    "}\n",
    "\n",
    "for weight_type in ['inverse', 'inverse_square']:\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(f\"Weight function: 1/d\" if weight_type == 'inverse' else f\"Weight function: 1/d²\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nEvaluating K={k}...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = weighted_knn_predict(X_train, y_train, X_test, k, weight_type)\n",
    "        \n",
    "        # Compute metrics\n",
    "        precision_dict, recall_dict, accuracy = compute_metrics(y_test, y_pred)\n",
    "        \n",
    "        weighted_results[weight_type][k] = {\n",
    "            'predictions': y_pred,\n",
    "            'precision': precision_dict,\n",
    "            'recall': recall_dict,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision per class: {precision_dict}\")\n",
    "        print(f\"  Recall per class: {recall_dict}\")\n",
    "\n",
    "# Visualization for weighted K-NN\n",
    "print(\"\\nGenerating visualizations for weighted K-NN...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Distance-Weighted K-NN Performance', fontsize=16)\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "ax = axes[0, 0]\n",
    "acc_standard = [standard_results[k]['accuracy'] for k in k_values]\n",
    "acc_inverse = [weighted_results['inverse'][k]['accuracy'] for k in k_values]\n",
    "acc_inverse_sq = [weighted_results['inverse_square'][k]['accuracy'] for k in k_values]\n",
    "ax.plot(k_values, acc_standard, 'o-', label='Standard', linewidth=2, markersize=8)\n",
    "ax.plot(k_values, acc_inverse, 's-', label='1/d', linewidth=2, markersize=8)\n",
    "ax.plot(k_values, acc_inverse_sq, '^-', label='1/d²', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('K Value')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(k_values)\n",
    "\n",
    "# 2. Precision for 1/d\n",
    "ax = axes[0, 1]\n",
    "for cls in species:\n",
    "    precisions = [weighted_results['inverse'][k]['precision'][cls] for k in k_values]\n",
    "    ax.plot(k_values, precisions, 'o-', label=cls, linewidth=2, markersize=8)\n",
    "ax.set_xlabel('K Value')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision per Class (1/d)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(k_values)\n",
    "\n",
    "# 3. Precision for 1/d²\n",
    "ax = axes[0, 2]\n",
    "for cls in species:\n",
    "    precisions = [weighted_results['inverse_square'][k]['precision'][cls] for k in k_values]\n",
    "    ax.plot(k_values, precisions, 'o-', label=cls, linewidth=2, markersize=8)\n",
    "ax.set_xlabel('K Value')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision per Class (1/d²)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(k_values)\n",
    "\n",
    "# 4. Recall for 1/d\n",
    "ax = axes[1, 0]\n",
    "for cls in species:\n",
    "    recalls = [weighted_results['inverse'][k]['recall'][cls] for k in k_values]\n",
    "    ax.plot(k_values, recalls, 'o-', label=cls, linewidth=2, markersize=8)\n",
    "ax.set_xlabel('K Value')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Recall per Class (1/d)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(k_values)\n",
    "\n",
    "# 5. Recall for 1/d²\n",
    "ax = axes[1, 1]\n",
    "for cls in species:\n",
    "    recalls = [weighted_results['inverse_square'][k]['recall'][cls] for k in k_values]\n",
    "    ax.plot(k_values, recalls, 'o-', label=cls, linewidth=2, markersize=8)\n",
    "ax.set_xlabel('K Value')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Recall per Class (1/d²)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(k_values)\n",
    "\n",
    "# 6. Average metrics comparison\n",
    "ax = axes[1, 2]\n",
    "avg_prec_inv = [np.mean(list(weighted_results['inverse'][k]['precision'].values())) for k in k_values]\n",
    "avg_rec_inv = [np.mean(list(weighted_results['inverse'][k]['recall'].values())) for k in k_values]\n",
    "avg_prec_inv_sq = [np.mean(list(weighted_results['inverse_square'][k]['precision'].values())) for k in k_values]\n",
    "avg_rec_inv_sq = [np.mean(list(weighted_results['inverse_square'][k]['recall'].values())) for k in k_values]\n",
    "ax.plot(k_values, avg_prec_inv, 'o-', label='Precision (1/d)', linewidth=2, markersize=8)\n",
    "ax.plot(k_values, avg_rec_inv, 's-', label='Recall (1/d)', linewidth=2, markersize=8)\n",
    "ax.plot(k_values, avg_prec_inv_sq, '^-', label='Precision (1/d²)', linewidth=2, markersize=8)\n",
    "ax.plot(k_values, avg_rec_inv_sq, 'v-', label='Recall (1/d²)', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('K Value')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Average Metrics Comparison')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(k_values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('1g_weighted_knn_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find best configurations\n",
    "best_k_inverse = max(k_values, key=lambda k: weighted_results['inverse'][k]['accuracy'])\n",
    "best_k_inverse_sq = max(k_values, key=lambda k: weighted_results['inverse_square'][k]['accuracy'])\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"BEST CONFIGURATIONS:\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(f\"Weighted K-NN (1/d):\")\n",
    "print(f\"  Best K: {best_k_inverse}\")\n",
    "print(f\"  Accuracy: {weighted_results['inverse'][best_k_inverse]['accuracy']:.4f}\")\n",
    "print(f\"\\nWeighted K-NN (1/d²):\")\n",
    "print(f\"  Best K: {best_k_inverse_sq}\")\n",
    "print(f\"  Accuracy: {weighted_results['inverse_square'][best_k_inverse_sq]['accuracy']:.4f}\")\n",
    "print()\n",
    "\n",
    "### 1.h. Comparison and Interpretation\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1(h): COMPARISON AND INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nSUMMARY TABLE - BEST K VALUES:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Method':<30} {'Best K':<10} {'Accuracy':<12} {'Avg Precision':<15} {'Avg Recall':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "methods = [\n",
    "    ('Standard K-NN', best_k_standard, standard_results[best_k_standard]),\n",
    "    ('Weighted K-NN (1/d)', best_k_inverse, weighted_results['inverse'][best_k_inverse]),\n",
    "    ('Weighted K-NN (1/d²)', best_k_inverse_sq, weighted_results['inverse_square'][best_k_inverse_sq])\n",
    "]\n",
    "\n",
    "for method_name, k, results in methods:\n",
    "    avg_prec = np.mean(list(results['precision'].values()))\n",
    "    avg_rec = np.mean(list(results['recall'].values()))\n",
    "    print(f\"{method_name:<30} {k:<10} {results['accuracy']:<12.4f} {avg_prec:<15.4f} {avg_rec:<15.4f}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Detailed comparison\n",
    "print(\"\\nDETAILED ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n1. STANDARD K-NN vs WEIGHTED K-NN (1/d):\")\n",
    "acc_diff_1 = weighted_results['inverse'][best_k_inverse]['accuracy'] - standard_results[best_k_standard]['accuracy']\n",
    "print(f\"   Accuracy difference: {acc_diff_1:+.4f}\")\n",
    "if acc_diff_1 > 0:\n",
    "    print(f\"   → Weighted K-NN (1/d) performs {abs(acc_diff_1)*100:.2f}% better\")\n",
    "elif acc_diff_1 < 0:\n",
    "    print(f\"   → Standard K-NN performs {abs(acc_diff_1)*100:.2f}% better\")\n",
    "else:\n",
    "    print(f\"   → Both methods perform equally\")\n",
    "\n",
    "print(\"\\n2. STANDARD K-NN vs WEIGHTED K-NN (1/d²):\")\n",
    "acc_diff_2 = weighted_results['inverse_square'][best_k_inverse_sq]['accuracy'] - standard_results[best_k_standard]['accuracy']\n",
    "print(f\"   Accuracy difference: {acc_diff_2:+.4f}\")\n",
    "if acc_diff_2 > 0:\n",
    "    print(f\"   → Weighted K-NN (1/d²) performs {abs(acc_diff_2)*100:.2f}% better\")\n",
    "elif acc_diff_2 < 0:\n",
    "    print(f\"   → Standard K-NN performs {abs(acc_diff_2)*100:.2f}% better\")\n",
    "else:\n",
    "    print(f\"   → Both methods perform equally\")\n",
    "\n",
    "print(\"\\n3. WEIGHTED K-NN (1/d) vs WEIGHTED K-NN (1/d²):\")\n",
    "acc_diff_3 = weighted_results['inverse'][best_k_inverse]['accuracy'] - weighted_results['inverse_square'][best_k_inverse_sq]['accuracy']\n",
    "print(f\"   Accuracy difference: {acc_diff_3:+.4f}\")\n",
    "if acc_diff_3 > 0:\n",
    "    print(f\"   → 1/d weighting performs {abs(acc_diff_3)*100:.2f}% better\")\n",
    "elif acc_diff_3 < 0:\n",
    "    print(f\"   → 1/d² weighting performs {abs(acc_diff_3)*100:.2f}% better\")\n",
    "else:\n",
    "    print(f\"   → Both weightings perform equally\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Determine overall best method\n",
    "all_methods = [\n",
    "    ('Standard K-NN', standard_results[best_k_standard]['accuracy'], best_k_standard),\n",
    "    ('Weighted (1/d)', weighted_results['inverse'][best_k_inverse]['accuracy'], best_k_inverse),\n",
    "    ('Weighted (1/d²)', weighted_results['inverse_square'][best_k_inverse_sq]['accuracy'], best_k_inverse_sq)\n",
    "]\n",
    "best_method = max(all_methods, key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n✓ BEST PERFORMING METHOD: {best_method[0]}\")\n",
    "print(f\"  K value: {best_method[2]}\")\n",
    "print(f\"  Accuracy: {best_method[1]:.4f}\")\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Distance weighting tends to give more importance to closer neighbors,\")\n",
    "print(\"   which can improve classification when nearby points are more relevant.\")\n",
    "print(\"\\n2. The 1/d² weighting scheme gives even more emphasis to very close points\")\n",
    "print(\"   compared to 1/d, which may help or hurt depending on data distribution.\")\n",
    "print(\"\\n3. Standard K-NN uses equal voting, which can be more robust when the\")\n",
    "print(\"   feature space is well-normalized and all neighbors are equally informative.\")\n",
    "print(\"\\n4. For the Iris dataset, the relatively simple decision boundaries may\")\n",
    "print(\"   not benefit significantly from distance weighting.\")\n",
    "\n",
    "# Per-class performance comparison\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PER-CLASS PERFORMANCE (BEST K VALUES):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for cls in species:\n",
    "    print(f\"\\n{cls}:\")\n",
    "    print(f\"  {'Method':<25} {'Precision':<12} {'Recall':<12}\")\n",
    "    print(f\"  {'-'*50}\")\n",
    "    for method_name, k, results in methods:\n",
    "        prec = results['precision'][cls]\n",
    "        rec = results['recall'][cls]\n",
    "        print(f\"  {method_name:<25} {prec:<12.4f} {rec:<12.4f}\")\n",
    "\n",
    "# Final visualization: Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Best K Comparison: Standard vs Weighted K-NN', fontsize=16)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[0]\n",
    "methods_names = ['Standard', '1/d', '1/d²']\n",
    "accuracies_best = [m[1] for m in all_methods]\n",
    "colors_bar = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "bars = ax.bar(methods_names, accuracies_best, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy Comparison')\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars, accuracies_best):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Precision per class\n",
    "ax = axes[1]\n",
    "x = np.arange(len(species))\n",
    "width = 0.25\n",
    "for i, (method_name, k, results) in enumerate(methods):\n",
    "    precisions = [results['precision'][cls] for cls in species]\n",
    "    ax.bar(x + i*width, precisions, width, label=method_name, color=colors_bar[i], alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Species')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision per Class')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(species, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Recall per class\n",
    "ax = axes[2]\n",
    "for i, (method_name, k, results) in enumerate(methods):\n",
    "    recalls = [results['recall'][cls] for cls in species]\n",
    "    ax.bar(x + i*width, recalls, width, label=method_name, color=colors_bar[i], alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Species')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Recall per Class')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(species, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('1h_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
